# 2023-数据科学导论实验报告

[TOC]

- **比赛名称**：[kaggle-Titanic](https://www.kaggle.com/competitions/titanic/overview)
- **队伍名**：以个人形式提交，用户名tinykevin
- **实验结果及比赛排名**：
  - 正确率：78.2%
  - 排名：3387/16266

## 问题定义

Titanic - Machine Learning from Disaster是一个经典的机器学习入门问题。该比赛中参赛者需要根据比赛给出的一位乘客的信息(姓名，性别，年龄，称呼，票价等)预测该乘客是否幸存。

## 做题思路
- **数据分析**：首先通过可视化的工具对训练集进行分析
- **特征提取**：通过第一步的分析提取特征
- **模型选择**：选择合适的模型对提取的特征学习并预测
- **调整参数**：对第三步选择的多种模型进行融合，根据训练的结果进行调优

### 数据分析

### 特征工程
在手动提取了许多数据集的特征之后对特征进行了一些挑选。首先对提取的特征之间的关联进行分析，热力图如下。

![correlation](../img/correlation_of_features.png)

我们通过该关联性的热力图可以看到许多和推测契合的特征关联性以及在特征提取中参考前人工作时一些觉得不合理但在关联性图中得到解释的特征。
比如Name_length这一特征在很多关于Kaggle-Titanic比赛中都有所提到。起初我们非常不解不过现在在进行特征间相关度分析后得到了解释。Name_length和性别有很大的关系，这就不难解释该特征在判断乘客是否存货中起到相当重要的作用。
另外如特征Cabin也就是舱位由于非常多样也有非常多的缺失，我们将是否拥有仓位作为特征Cabin在关联热力图中我们发现

### 模型简介
本次实验使用了较为常见的分类器并且对其单独预测以及融合进行了评估。在本次实验中使用了多种分类器，分别是决策树、随机森林、极端随机树、adaboost、最近邻、逻辑回归以及支持向量机，下面对各个模型进行简单的介绍。

#### 决策树
决策树是一种用于分类和回归的监督学习算法。它通过对数据集进行递归地划分，以创建一个树形结构，其中每个内部节点表示一个特征或属性上的测试，每个分支代表一个测试输出，而每个叶节点代表一个类别标签或者一个数值。在分类问题中，决策树通过对特征进行划分来逐步确定最终的类别；在回归问题中，决策树通过对特征进行划分来逐步确定最终的数值。决策树时一种非常符合直觉的模型，并且较善于处理数值类别的特征。
#### 随机森林
随机森林分类器是一种集成学习方法，它由多个决策树组成。在训练过程中，每棵决策树都会使用随机选择的特征和样本来进行训练，这样可以减少过拟合的风险。在进行分类时，随机森林会将每棵树的分类结果进行投票，然后选择投票最多的类别作为最终的分类结果，对于处理缺失数据和异常值也有一定的鲁棒性
#### 极端随机树
类似于随机森林，极端随机树也是一种集成学习的方法。它也由多棵决策树组成，但在构建每棵树的过程中，它使用了更多的随机性。具体来说，对于每个节点的特征划分，极端随机树会随机选择特征的划分阈值，而不是像随机森林那样通过对特征的随机子集进行划分。这种方法可以降低模型的方差，加快训练速度，并且对于噪声数据更加鲁棒。
#### AdaBoost
同样也是一种集成学习的方法。AdaBoost通过反复训练一系列弱分类器（比如决策树或者神经网络），每一次训练都会调整数据的权重，使得之前分类错误的样本在下一次训练中得到更多的关注。最终，Adaboost会将这些弱分类器组合成一个强分类器，通过对每个弱分类器的结果进行加权投票来进行最终的分类。
#### GradientBoost
Gradient Boosting分类器是一种集成学习方法，它通过迭代地训练一系列弱分类器（通常是决策树），每一次训练都会根据上一轮的预测结果来调整数据的权重，以便更好地拟合残差。最终，这些弱分类器会被组合成一个强分类器，通过对每个弱分类器的结果进行加权求和来进行最终的预测。
#### SVM
向量支持机是一种用于分类和回归的监督学习算法。在分类问题中，SVM的目标是找到一个最优的超平面，能够将不同类别的样本分开。在回归问题中，SVM的目标是找到一个超平面，使得训练数据点到该超平面的距离尽可能地小。之后通过超平面将向量构成的超空间分为多个子空间，被分到不同子空间的样本可以看作一类。
#### 逻辑回归
逻辑回归模型是最常用的二分类模型，其通过一个逻辑函数（也称为Sigmoid函数）来对样本进行分类。逻辑回归模型的输出是一个介于0和1之间的概率值，表示样本属于某个类别的概率。当这个概率值大于0.5时，样本被划分为正类；当概率值小于0.5时，样本被划分为负类。
#### 最近邻
决策树是一种用于分类和回归的监督学习算法。它通过对数据集进行递归地划分，以创建一个树形结构，其中每个内部节点表示一个特征或属性上的测试，每个分支代表一个测试输出，而每个叶节点代表一个类别标签或者一个数值。在分类问题中，决策树通过对特征进行划分来逐步确定最终的类别。

另外我们还使用了简单的投票器以及bagging两种模型融合方式进行训练。以下为两种模型融合的简介。

#### Voting
简单地把多个模型的投票进行加权平均，在本次实验中可以看作选取判断为生存或者遇难模型数更多的结果。

#### Bagging
采用有放回的方式进行抽样，用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合。

### 模型评估
首先我们通过k-fold方法对上述所有模型进行了评估(k-fold值为10)，其中评估的平均得分如下
![evaluation](../img/evaluation_of_models.png)

仅从给定的训练集中看最随机森林的效果是最好的，另外我们对各个模型的学习曲线进行观察，可以看出有部分模型训练效果较好，但是有些模型如决策树等模型仍处于欠拟合的状态，于是我们尝试对参数进行调整取得了一些效果。不过由于是调用sklearn函数的原因最后也没有取得非常好的成效。
<figure>
<img src="../img/ada_curve.png" width=200/>
<img src="../img/dt_curve.png" width=200/>
<img src="../img/et_curve.png" width=200/>
</figure>

<figure>
<img src="../img/knn_curve.png" width=200/>
<img src="../img/lg_curve.png" width=200/>
<img src="../img/rf_curve.png" width=200/>
</figure>
<figure>
<img src="../img/svm_curve.png" width=200/>
<img src="../img/gradientboost_curve.png" width=200/>
</figure>

### 模型调参

## 实验分工与个人感悟

### 实验分工

### 个人感悟——宋林恺

### 个人感悟——杨神

### 个人感悟——罗神

