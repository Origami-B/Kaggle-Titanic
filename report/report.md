# 2023-数据科学导论实验报告

[TOC]

- **比赛名称**：[kaggle-Titanic](https://www.kaggle.com/competitions/titanic/overview)
- **队伍名**：以个人形式提交，用户名tinykevin
- **实验结果及比赛排名**：
  - 正确率：78.2%
  - 排名：3387/16266

## 问题定义

Titanic - Machine Learning from Disaster是一个经典的机器学习入门问题。该比赛中参赛者需要根据比赛给出的一位乘客的信息(姓名，性别，年龄，称呼，票价等)预测该乘客是否幸存。

## 做题思路
- **数据分析**：首先通过可视化的工具对训练集进行分析
- **特征提取**：通过第一步的分析提取特征
- **模型选择**：选择合适的模型对提取的特征学习并预测
- **调整参数**：对第三步选择的多种模型进行融合，根据训练的结果进行调优

### 数据分析

#### 一、原始数据分析
观察数据集，发现数据集中有以下几个特征：
|特征|含义|
|:---:|:---:|
|PassengerId|乘客编号|
|Survived|是否幸存|
|Pclass|船票等级|
|Name|乘客姓名|
|Sex|乘客性别|
|Age|乘客年龄|
|SibSp|亲戚数量（兄妹、配偶数）|
|Parch|亲戚数量（父母、子女数）|
|Ticket|船票号码|
|Fare|船票价格|
|Cabin|船舱|
|Embarked|登船港口|

使用命令`df.info()`查看数据集的基本信息，结果如下：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB

```

可以发现，数据集中如下的特点：
- 共有891条数据，数据量较小，训练用时不会太长。
- 特征存在缺失值需要完成补全、修改或舍弃。
- 可能存在无关特征，如名字、称呼等
- 特征中存在字符串，如有需要可转换为数值类型
- 数据集中特征的取值范围不一，如年龄的取值范围为0-80，而船票价格的取值范围为0-512，这可能会影响模型的训练效果，需要进行归一化处理。
- 数据集特征的值不能完全反映乘客的生存情况，考虑新的特征，如家庭成员数、家庭成员是否全部遇难等。

#### 二、数据可视化分析

> 显然，我们不能直接将原始数据输入模型进行训练，需要对数据进行预处理，包括数据清洗、特征提取、特征选择、特征变换等。在进行数据预处理之前，我们需要对数据进行可视化分析，以便更好地理解数据，为后续的数据预处理提供依据。

##### 1. 性别与生存率的关系
作出性别(sex)与生存率的柱状图如下：
![](../img/sex-survived.png)

从图中可以看出，女性的生存率明显高于男性，可能是 **Lady First**的观点导致的，那么预测时可以将sex作为一个重要的特征。

##### 2. 船票等级与生存率的关系
作出船票等级(Pclass)与生存率的柱状图如下：
![](../img/pclass-survived.png)

从图中可以看出，船票等级越高，生存率越高，可能是因为船票等级越高，船舱越好，逃生时的优先级越高，那么预测时可以将Pclass作为一个重要的特征。

##### 3. 年龄与生存率的关系
作出年龄(Age)与生存率的分布图如下：
![](../img/age-survived.png)

从图中可以看出，不同年龄的生存率差异较大，儿童的生存率较高，而青壮年的生存率较低，可能是因为儿童的逃生优先级较高，而青壮年的逃生优先级较低。

另外，作出不同年龄人群的生存率的直方图如下：
![](../img/diff-age-survived.png)

上图也从另一方面反映了个年龄人群的生存率差异较大，那么预测时可以将Age作为一个重要的特征。

##### 4. 家庭情况与生存率的关系
作出家庭成员数(SibSp+Parch)与生存率的饼状图如下：
![](../img/sibsp-survived.png)
![](../img/parch-survived.png)

可以看到有无家庭成员的生存率差异较大，但是仅仅考虑有无家庭成员显然是不够的，因为有家庭成员的人数不同，生存率也不同，所以我们将家庭成员数(SibSp+Parch+1)作为一个新的特征进行进一步的分析，作出相关柱状图如下：
![](../img/family-num-survived.png)

同我们之前的分析，家庭成员数也较为明显的影响了生存率，那么预测时可以将家庭成员数作为一个重要的特征。

##### 5. 票价与生存率的关系
作出票价(Fare)的分布以及不同船票等级与票价的关系如下：
![](../img/fare.png)
![](../img/fare-pclass.png)

作出生存与否人群的票价均值和方差如下：
![](../img/fare-survived.png)

从上图可以看出，存活与非存活人群间的票价均值和方差差异较大，另一方面，票价与船票等级有较大的关系，所以预测时Fare与Pclass的重要性需要权衡。

##### 6. 登船港口与生存率的关系
作出登船港口(Embarked)与生存人数的柱状图如下：
![](../img/embarked-survived.png)

从图中可以看出，登船港口与生存率有一定的关系，原因可能是不同港口登船的人对船的熟悉程度存在差异，因此预测时Embarked也可以作为一个重要的特征。

##### 7. 名字与生存率的关系
直觉上名字(Name)与生存率没有关系，但是由于严谨性，我们将名字的长度作为一个特征，作出名字长度与生存率的直方图如下：
![](../img/name-len-survived.png)

令人吃惊的是，名字长度与生存率也存在一定的关系，所以我们将名字长度也作为一个重要特征备选项。

##### 8. 船舱与生存率的关系
船舱特征的最大问题在于缺失值太多，无法作出有效的填充，但是我们本着最大化利用数据集的原则，将船舱特征进行了额外的处理，将船舱特征分为有船舱和无船舱两类，作出有无船舱与生存率的柱状图如下：
![](../img/cabin-survived.png)

从图中可以看出，有无船舱信息与生存率也存在一定的关系，所以我们将有无船舱也作为一个重要特征。

##### 9. 称呼与生存率的关系
作出称呼与生存率的柱状图如下：
![](../img/title-survived.png)

从图中可以看出，称呼与生存率也存在一定的关系，这可能是因为称呼实际上可以反映一部分的个人信息，如：性别、婚姻、职业等，所以预测时Title也可以作为一个重要的特征。

##### 可视化分析总结
我们对数据集的特征做了可视化分析，发现Sex、Pclass、Age、FamilyNum、Fare、Embarked、NameLen、Cabin、Title，这些特征都与生存率有一定的关系，这几乎利用了全部的数据集信息。在这些特征中，一些是符合直觉的，还有一些是不符合直觉的，但是我们都可以从可视化分析中得到这些特征与生存率的关系，显然说明了数据可视化分析的重要性。

而之后我们将进行数据预处理，将这些特征进行提取、变换、选择等，以便更好地训练模型。

### 特征工程
在手动提取了许多数据集的特征之后对特征进行了一些挑选。首先对提取的特征之间的关联进行分析，热力图如下。

![correlation](../img/correlation_of_features.png)

我们通过该关联性的热力图可以看到许多和推测契合的特征关联性以及在特征提取中参考前人工作时一些觉得不合理但在关联性图中得到解释的特征。
比如Name_length这一特征在很多关于Kaggle-Titanic比赛中都有所提到。起初我们非常不解不过现在在进行特征间相关度分析后得到了解释。Name_length和性别有很大的关系，这就不难解释该特征在判断乘客是否存货中起到相当重要的作用。
另外如特征Cabin也就是舱位由于非常多样也有非常多的缺失，我们将是否拥有仓位作为特征Cabin在关联热力图中我们发现

### 模型简介
本次实验使用了较为常见的分类器并且对其单独预测以及融合进行了评估。在本次实验中使用了多种分类器，分别是决策树、随机森林、极端随机树、adaboost、最近邻、逻辑回归以及支持向量机，下面对各个模型进行简单的介绍。

#### 决策树
决策树是一种用于分类和回归的监督学习算法。它通过对数据集进行递归地划分，以创建一个树形结构，其中每个内部节点表示一个特征或属性上的测试，每个分支代表一个测试输出，而每个叶节点代表一个类别标签或者一个数值。在分类问题中，决策树通过对特征进行划分来逐步确定最终的类别；在回归问题中，决策树通过对特征进行划分来逐步确定最终的数值。决策树时一种非常符合直觉的模型，并且较善于处理数值类别的特征。
#### 随机森林
随机森林分类器是一种集成学习方法，它由多个决策树组成。在训练过程中，每棵决策树都会使用随机选择的特征和样本来进行训练，这样可以减少过拟合的风险。在进行分类时，随机森林会将每棵树的分类结果进行投票，然后选择投票最多的类别作为最终的分类结果，对于处理缺失数据和异常值也有一定的鲁棒性
#### 极端随机树
类似于随机森林，极端随机树也是一种集成学习的方法。它也由多棵决策树组成，但在构建每棵树的过程中，它使用了更多的随机性。具体来说，对于每个节点的特征划分，极端随机树会随机选择特征的划分阈值，而不是像随机森林那样通过对特征的随机子集进行划分。这种方法可以降低模型的方差，加快训练速度，并且对于噪声数据更加鲁棒。
#### AdaBoost
同样也是一种集成学习的方法。AdaBoost通过反复训练一系列弱分类器（比如决策树或者神经网络），每一次训练都会调整数据的权重，使得之前分类错误的样本在下一次训练中得到更多的关注。最终，Adaboost会将这些弱分类器组合成一个强分类器，通过对每个弱分类器的结果进行加权投票来进行最终的分类。
#### GradientBoost
Gradient Boosting分类器是一种集成学习方法，它通过迭代地训练一系列弱分类器（通常是决策树），每一次训练都会根据上一轮的预测结果来调整数据的权重，以便更好地拟合残差。最终，这些弱分类器会被组合成一个强分类器，通过对每个弱分类器的结果进行加权求和来进行最终的预测。
#### SVM
向量支持机是一种用于分类和回归的监督学习算法。在分类问题中，SVM的目标是找到一个最优的超平面，能够将不同类别的样本分开。在回归问题中，SVM的目标是找到一个超平面，使得训练数据点到该超平面的距离尽可能地小。之后通过超平面将向量构成的超空间分为多个子空间，被分到不同子空间的样本可以看作一类。
#### 逻辑回归
逻辑回归模型是最常用的二分类模型，其通过一个逻辑函数（也称为Sigmoid函数）来对样本进行分类。逻辑回归模型的输出是一个介于0和1之间的概率值，表示样本属于某个类别的概率。当这个概率值大于0.5时，样本被划分为正类；当概率值小于0.5时，样本被划分为负类。
#### 最近邻
决策树是一种用于分类和回归的监督学习算法。它通过对数据集进行递归地划分，以创建一个树形结构，其中每个内部节点表示一个特征或属性上的测试，每个分支代表一个测试输出，而每个叶节点代表一个类别标签或者一个数值。在分类问题中，决策树通过对特征进行划分来逐步确定最终的类别。

另外我们还使用了简单的投票器以及bagging两种模型融合方式进行训练。以下为两种模型融合的简介。

#### Voting
简单地把多个模型的投票进行加权平均，在本次实验中可以看作选取判断为生存或者遇难模型数更多的结果。

#### Bagging
采用有放回的方式进行抽样，用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合。

### 模型评估
首先我们通过k-fold方法对上述所有模型进行了评估(k-fold值为10)，其中评估的平均得分如下
![evaluation](../img/evaluation_of_models.png)

仅从给定的训练集中看最随机森林的效果是最好的，另外我们对各个模型的学习曲线进行观察，可以看出有部分模型训练效果较好，但是有些模型如决策树等模型仍处于欠拟合的状态，于是我们尝试对参数进行调整取得了一些效果。不过由于是调用sklearn函数的原因最后也没有取得非常好的成效。
<figure>
<img src="../img/ada_curve.png" width=200/>
<img src="../img/dt_curve.png" width=200/>
<img src="../img/et_curve.png" width=200/>
</figure>

<figure>
<img src="../img/knn_curve.png" width=200/>
<img src="../img/lg_curve.png" width=200/>
<img src="../img/rf_curve.png" width=200/>
</figure>
<figure>
<img src="../img/svm_curve.png" width=200/>
<img src="../img/gradientboost_curve.png" width=200/>
</figure>

### 模型调参

## 实验分工与个人感悟

### 实验分工

### 个人感悟——宋林恺

### 个人感悟——杨神

### 个人感悟——罗神

